{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Localización y clasificación de objetos en video con YOLO**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instalar onnx.**\n",
    "Herramienta para modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Librerías.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particular usaremos extensivamente el paquete y funciones ```cv2.dnn``` que permiten implementar yolo de una manera muy fácil y sencilla en cualquier aplicación escrita en Python donde se trabaje con imágenes. \n",
    "\n",
    "Primero necesitamos cargar los nombres de las clases en las que se ha entrenado la red. Ten en cuenta que la red que utilicemos solo podrá reconocer las clases en las que se ha formado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsPath = \"./yolo-coco/coco.names\"\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'bicycle', 'car', 'motorbike', 'aeroplane']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora generaremos un color para cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar semilla de números pseudo-aleatorios\n",
    "np.random.seed(42)\n",
    "# Color para cada clase\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos el color de cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[102, 220, 225],\n",
       "       [ 95, 179,  61],\n",
       "       [234, 203,  92],\n",
       "       [  3,  98, 243],\n",
       "       [ 14, 149, 245]], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLORS[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos nuestro modelo de Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n"
     ]
    }
   ],
   "source": [
    "modelPath = \"./yolo-coco/yolov5s.onnx\"\n",
    "\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNet(modelPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos el video que queremos analizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('./videos/test/soccer.mp4')\n",
    "writer  = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinamos el número total de frames en el video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "frames = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() \\\n",
    "\t\telse cv2.CAP_PROP_FRAME_COUNT\n",
    "total = int(video.get(frames))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recorremos los frames del video y analizamos frame por frame, guardamos las etiquetas y marcos de los objetos detectados en el mismo video, generando otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\t# Leer el siguiente frame\n",
    "\t(r, frame) = video.read()\n",
    "\t# Si el frame no fue leido, salimos del loop\n",
    "\tif not r:\n",
    "\t\tbreak\n",
    "\n",
    "\t# Construimos un blob desde el marco de entrada y luego realizamos un envío de este\n",
    "\t# hacía el detector de objetos (YOLO). Para construir los marcos de salida, y las probabilidades\n",
    "\t# Necesitamos cambiar el tamaño de la imagen a un cuadrado de 416x416 píxeles y hacer un pase hacia adelante a través de la red.\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (640, 640), swapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tstart = time.time()\n",
    "\tlayerOutputs = net.forward()\n",
    "\tend = time.time()\n",
    "\n",
    "\t# Inicializamos la lista de marcos de salida detectados, las clases y las probabilidades respectivamente\n",
    "\tclass_ids = []\n",
    "\tconfidences = []\n",
    "\tboxes = []\n",
    " \n",
    "\toutput_data = layerOutputs[0]\n",
    "\n",
    "\timage_width, image_height, _ = frame.shape\n",
    "\tx_factor = image_width / 640\n",
    "\ty_factor =  image_height / 640\n",
    "\n",
    "\t# Hacemos un bucle para cada una de las detecciones\n",
    "\tfor r in range(len(output_data)):\n",
    "\t\trow = output_data[r]\n",
    "\t\tconfidence = row[4]\n",
    "\n",
    "\t\tif confidence >= 0.4:\n",
    "\n",
    "\t\t\tclasses_scores = row[5:]\n",
    "\t\t\t_, _, _, max_indx = cv2.minMaxLoc(classes_scores)\n",
    "\t\t\tclass_id = max_indx[1]\n",
    "\t\t\tif (classes_scores[class_id] > .25):\n",
    "\t\t\t\t\"\"\"Escalar las coordenadas del cuadro delimitador hacia atrás en relación con el\n",
    "\t\t\t\ttamaño de la imagen, teniendo en cuenta que YOLO en realidad\n",
    "\t\t\t\tdevuelve las coordenadas del centro (x, y) del límite\n",
    "\t\t\t\tcuadro seguido del ancho y alto de los cuadros.\"\"\"\n",
    "\t\t\t\t\n",
    "\t\t\t\tconfidences.append(confidence)\n",
    "\t\t\t\tclass_ids.append(class_id)\n",
    "\n",
    "\t\t\t\tx, y, w, h = row[0].item(), row[1].item(), row[2].item(), row[3].item() \n",
    "\t\t\t\t\n",
    "\t\t\t\tleft = int((x - 0.5 * w) * x_factor)\n",
    "\t\t\t\ttop = int((y - 0.5 * h) * y_factor)\n",
    "\t\t\t\twidth = int(w * x_factor)\n",
    "\t\t\t\theight = int(h * y_factor)\n",
    "\t\t\t\t\n",
    "\t\t\t\tbox = np.array([left, top, width, height])\n",
    "\t\t\t\tboxes.append(box)\n",
    "\n",
    "\t# aplicar supresión no máxima para suprimir cuadros delimitadores débiles y superpuestos\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.7, 0.3)\n",
    "\n",
    "\t# asegúrese de que exista al menos una detección\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# bucle sobre los índices que estamos manteniendo\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extraer las coordenadas del cuadro delimitador\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "\t\t\t# dibuje un rectángulo de cuadro delimitador y una etiqueta en la imagen\n",
    "\t\t\tcolor = [int(c) for c in COLORS[class_ids[i]]]\n",
    "\t\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\t\t\ttext = \"{}: {:.4f}\".format(LABELS[class_ids[i]], confidences[i])\n",
    "\t\t\tcv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\t# Si el video writer no está inicializado, inicializamos uno\n",
    "\tif writer is None:\n",
    "\t\t# Inicializamos nuestro video writer\n",
    "\t\tformat = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\t\twriter = cv2.VideoWriter(\"./videos/detection/video_soccer_detection.mp4\", format, 30, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "\t# Guardamos el video con las etiquetas y los marcos de salida\n",
    "\twriter.write(frame)\n",
    "writer.release()\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110fe3fb9777db4ce1f884af3cc527a40b2c98427ad17781c021ef692bd3d28d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
